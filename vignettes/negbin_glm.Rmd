---
title: "negbin_glm"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{negbin_glm}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
#require(MASS)
library(adaglm)
```

Simulate data under NB GLM. 
```{r}
N = 1000
P = 10
X = matrix(rnorm(N*P), N, P)
true_conc = 5. 
true_b = rnorm(P)
offs = rnorm(N)
y = MASS::rnegbin(N, mu = exp(X %*% true_b + offs), theta = true_conc)
df = as.data.frame(X)
df$y = y
df$offs = offs
head(df)
```

Fit to obtain `glm` object. 
```{r}
my_nb_fit = adaglm(
  y ~ . - offs + offset(offs), 
  data = df,
  batch_size = 200,
  verbosity = 2)
```

```{r}
plot(c(0, true_b), my_nb_fit$coefficients, ylab = "Estimated coefficients", xlab = "True coefficients")
```

How well did we estimate the concentration (1/dispersion) parameter? 

```{r}
my_nb_fit$theta # true is 5. 
```

Summary gives statistical significance for each covariate
```{r}
summary(my_nb_fit) # just "summary" should work here but gets 
```

Use a likelihood ratio test to determine significance of one covariate (V1). Call this "reduced" to distinguish from "null" being intercept + offsets only. 
```{r}
my_nb_fit_reduced = adaglm(
  y ~ . - offs + offset(offs) - V1, 
  data = df, 
  batch_size = 200,
  verbosity = 2)
summary(my_nb_fit_reduced)
```

```{r}
anova(my_nb_fit,my_nb_fit_reduced)
```
Calculate Pearson residuals ala sctransform. These can be used as a convenient continuous representation of the data that are close to N(0,1) distributed
```{r}
pearson_resid = residuals(my_nb_fit)
hist(pearson_resid, 100)
```

```{r}
pvalues = pointwise_pvalues(my_nb_fit, y)
hist(pvalues$p_two_sided)
```

```{r}
R2deviance = 1 - my_nb_fit$deviance / my_nb_fit$null.deviance
R2deviance
```
All the covariates together explain 92% of variance

How much does V1 explain of the covariate-explainable variance? 
```{r}
my_nb_fit_reduced = adaglm(
  y ~ . - offs + offset(offs) - V1 - 1, 
  data = df, 
  conc = my_nb_fit$theta,
  batch_size = 200,
  verbosity = 2)

(my_nb_fit_reduced$deviance - my_nb_fit$deviance) / (my_nb_fit$null.deviance - my_nb_fit$deviance)
```

How much does V1 explain of the non-NB-sampling variance? 
```{r}
(my_nb_fit_reduced$deviance - my_nb_fit$deviance) / my_nb_fit$null.deviance
```

This is McFadden's R2 which makes little sense to me: 
```{r}
1-my_nb_fit$twologlik/my_nb_fit_reduced$twologlik
```

I think this could come out greater than 1 for continuous data! 
